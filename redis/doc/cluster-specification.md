[Redis集群规范](https://redis.io/topics/cluster-spec)
---
欢迎使用Redis集群规范，。在这里你可以找到Redis集群的算法和设计原理相关信息。这个文档是与集群的实现保持同步的。

## 设计的主要性质和原理

### 集群的目标

Redis集群是Redis的分布式实现，有如下目标，按照设计的重要性排序：

* 高性能和线性扩展到1000节点。没有代理，异步复制被使用，也没有对值执行合并的操作。

* 可接受的写安全程度:系统尝试(以最大努力的方式)保留来自与大多数主节点连接的客户端的所有写操作。通常有一些小窗口期，可能丢失已经确认的写入。当客户端位于少数分区时，丢失已经确认的写入的窗口会更大。

* 可用性:Redis集群能够在大多数主节点都可以访问的分区中生存，对于不能访问的主节点至少要有有一个可以访问的从节点。使用副本漂移，可以将副本从拥有多个副本的主节点下迁移到其它主节点。

本文档中描述的是在Redis 3.0或更高版本中实现的。

## 实现的子集

Redis集群实现了非分布式Redis支持的所有单key命令。执行复杂操作(如集合类型联合或交集)的命令也会被实现，只要这些key被hash到同一个slot。

Redis集群提出了一种hash tag的概念，用于强制将一些key存储到相同的slot。在手动重新分片期间，多key操作可能一段时间不可用，但是单key操作是一直可用的。

Redis集群不支持多个数据库。只有数据库0，不允许使用select命令。

## 客户端与服务端在Redis集群协议中的角色

Redis集群中的节点负责存储数据，获取集群的状态，将key映射到正确的节点上。集群节点可以自动发现其它节点，检测故障节点，当故障发生时为了将从节点提升为主节点。

为了执行任务集群中的所有节点都通过Tcp总线和二进制协议连接，称为Redis集群总线。通过集群总线每个节点点都连接上了其它节点。节点使用gossip协议传播关于集群消息，以便发现新的节点，发送ping包以确保所有其它节点都正常工作，发送集群消息来通知特定的条件。集群总线还用于在集群中传播发布/订阅消息和协调用户发起的手动failoveri请求。

由于集群节点不能代理请求，使用重定向错误Move和ASK可以将client重定向其它节点。理论上client可以将集群中的所有节点发送请求，如果需要会进行重定向，因此客户机不需要保持集群的状态，但是client可以缓存slot与节点的映射关系提高性能。

## 安全写入

Redis集群主从节点间使用异步复制，最后选举为master节点拥有的数据将替换所有其它从节点的数据。总有一段窗口期可能在分区期间丢失写操作。客户端连接大多数主节点和连接少部分主节点时的窗口期是不同的。

与拥有少数主节点的分区侧的写操作相比，Redis集群努力保留连接大多数主节点的client的写操作。以下是一些场景的示例，这些场景会导致在故障期间丢失接收到的已确认的写操作:

1. 写操作到达master节点，但是由于主从之间是异步复制的，在master回复客户端之后，数据可能没有传播到从节点,如果此时master挂了，从节点被提升为master数据将丢失。
2. 另外一种理论上可能导致数据丢失的情况如下:

* 由于分区主节点不能访问
* 由于从节点的原因导致主节点挂了
* 一段时间后master又能访问了
* client缓存了旧的路由关系，可能写数据到旧的主节点，在其转为从节点之前

一个master节点要进行failover，必须是大部分master节点在至少NODE_TIMEOUT内认为它不可达。如果在这段时间内分区恢复则不会丢失写数据。当分区持续超过NODE_TIMEOUT，在少数侧执行的写数据可能会丢失。当经过NODE_TIMEOUT后还没有连接上集群，少读侧节点将拒绝写操作，因此在少数侧节点不可再用前有一个最大窗期。

## 可用性

Redis集群在分区的少数端不可用。在分区的多数侧假设至少有大多数的master和每个不可达的master至少有一个salve，那么经过NODE_TIMEOUT加从节点选举为master花费的时间后集群将重新可用。

Redis集群被设计成在集群中少数节点故障时依然能正常服务，但是对于在发生大量网络分区时依然能够使用的场景这不是一种合适的解决方案。

由N个maste节点，每个master有一个salve组成的集群，如果只有一个节点被分区集群将继续可用。当有2个节点分区时，集群可用的概率1-(1/N*2)。(只有一个节点失败时剩余N\*2-1个节点，正好没有从节点的master失败的概率为1/(N\*2-1)))。

例如，集群中有个5个节点每个节点有一个从节点,两个节点与集群分区的概率为1/(5*2-1)=11.11%，分区后集群将不可以用。

由于有副本漂移机制，在许多实际的场景中，通过该机制将副本迁移到孤立的master节点，集群的可用性得到了提升。因此在每次成功的失败事件后，集群都可以进行重新配置从属布局，以便更好的抵抗下一次失败。

### 性能

Redis集群不代理命令到key所在的节点，而是重定向client到正确的节点。

最终client获得了节点与slot之间的关系。因此正常情况下client可以直接连接到正确的节点。

因为使用异步复制，节点不对等待其他的从节点的回应，如果没有显式使用WAIT命令。

多key操作都是在同一个节点上进行，除非进行了重新分片否则key不会出现在多个节点上。

正常情况下操作与单个实例的操作完全相同。这意味着，在具有N个主节点的Redis集群中，当设计线性扩展时，您可以期望与单个Redis实例乘以N的性能相同。同时，查询通常在一次往返中执行，因为客户机通常保留与节点的持久连接，因此，延迟数据也与单个独立Redis节点的情况相同。非常高的性能和可扩展性，同时保持弱而合理的数据安全性和可用性是Redis集群的主要目标。

### 为什么要避免合并操作

Redis集群设计避免了在多个节点中同一个键值对的冲突版本。Redis中的值通常非常大；通常会看到包含数百万个元素的列表或排序集。数据类型在语义上也很复杂。传输和合并这些类型的值可能是一个主要的瓶颈，并且/或者可能需要应用程序端逻辑的大量参与、存储元数据的额外内存等等。

### Redis集群主要组件概述

## key分配模型

键空间分割成16384个slot，有效的集群最大节点限制也是16384(建议最大节点在1000个左右)。

每个master节点负责16384个slot的一部分。当没有发生重新分片时slot的分配是稳定的。

映射key到slot的算法如下:

    HASH_SLOT = CRC16(key) mod 16384

CRC16算法规定如下:

* 算法名称:XMODEM(也称为XMODEM或CRC-16/ACORN)
* 宽度:16个字节
* Poly: 1021 (That is actually x16 + x12 + x5 + 1)
* 初始值:0000
* Reflect Input byte: False
* Reflect Output CRC: False
* Xor constant to output CRC: 0000
* Output for "123456789": 31C3

CRC16有14位bit的输出(这就是为什么在上面的公式中有一个16384模运算)。在我们的测试中，CRC16在16384插槽上均匀分布不同类型的key方面表现得非常好。

注意:本文附录A中提供了所用CRC16算法的参考实现。

### key hash tag

hash tag是一种实现多一个key分配到同一个slot的方法。hash tag会使用key的"{..}"部分进行计算crc16的值而不是使用整个key进行计算。

    unsigned int HASH_SLOT(char *key, int keylen) {
        int s, e; /* start-end indexes of { and } */

        /* Search the first occurrence of '{'. */
        for (s = 0; s < keylen; s++)
            if (key[s] == '{') break;

        /* No '{' ? Hash the whole key. This is the base case. */
        if (s == keylen) return crc16(key,keylen) & 16383;

        /* '{' found? Check if we have the corresponding '}'. */
        for (e = s+1; e < keylen; e++)
            if (key[e] == '}') break;

        /* No '}' or nothing between {} ? Hash the whole key. */
        if (e == keylen || e == s+1) return crc16(key,keylen) & 16383;

        /* If we are here there is both a { and a } on its right. Hash
        * what is in the middle between { and }. */
        return crc16(key+s+1,e-s-1) & 16383;
    }

### 集群节点属性

每一个节点都要一个唯一的名字node id。节点名是160位随机数的十六进制表示的，在节点启动时生成(通常使用/dev/urandom)。节点将名字保存到配置文件中，之后将一直使用这个名字，除非配置文件被删除或使用了CLUSTEER RESET命令，名称才会变换。

node id用于在整个集群中唯一标识每个节点。节点改变ip地址时不用改变它的node id。集群通过gossip协议可以检测到ip/por和配置的变化。

集群中的节点还有很多其它属性，有些是只需要在本节点保存的如果最后一次ping的时间，有些是需要集群中的不同节点间进行同步的。

每个节点维护集群中其它节点的以下信息:node ID、IP和port、一些falg、具有slave标记的节点的master、最近一次ping时间、最近一次手动pong时间、当前config epoch、连接状态和拥有的slot。

节点相关字段的详细信息可以参见CLUSTER NODES的文档

CLUSTER NODES命令可以发送到集群中的任何节点，返回从该节点的角度所看到的集群信息。

下面是发送CLUSTER NNDOES命令到拥有3个节点的集群发回的信息:

    $ redis-cli cluster nodes
    d1861060fe6a534d42d8a19aeb36600e18785e04 127.0.0.1:6379 myself - 0 1318428930 1 connected 0-1364
    3886e65cc906bfd9b1f7e7bde468726a052d1dae 127.0.0.1:6380 master - 1318428930 1318428931 2 connected 1365-2729
    d289c575dcbc4bdd2931585fd4339089e461a27d 127.0.0.1:6381 master - 1318428931 1318428931 3 connected 2730-4095

上面的例子发回的字段依次是:node id、ip/port、flag、last ping sent、last pong reveived、configuration epoch、link state、slots。当我们谈到Redis集群的特定部分时，将讨论关于上述字段的详细信息

### 集群总线

每个集群节点有一个额外的端口用于接收集群中其它节点的连接。这个端口和正常端口间有一个固定的10000偏移。假如正常端口是6379则集群总线使用的端口是16379。

节点与节点之间的通信只会通过集群总线和集群总线协议进行，集群总线协议是由不同类型和大小的帧组成的二进制协议。集群总线二进制协议没有公开文档，因为它不打算让外部软件设备使用该协议与Redis集群节点通信。通过阅读Redis集群源代码中的Cluster.h和Cluster.c文件，你可以获得关于集群总线协议的更多细节。

### 集群拓扑

Redis集群是一个完整的网格，其中每个节点都使用TCP连接与其它节点连接。

在一个有N个节点的集群中，每个节点都有N-1个传出的TCP连接和N-1个传入的连接。当一个节点需要一个pong应答来响应集群总线中的ping时，在等待足够长的时间将节点标记为不可到达之前，它将尝试通过从头重新连接来刷新与节点的连接。

虽然Redis集群节点构成一个完整的网格，但节点使用gossip协议和配置更新机制，以避免在正常情况下节点之间交换太多消息，因此交换的消息数量不是指数级的。

### 节点握手

节点总是在集群总线端口上接受连接，即使ping节点不受信任，也会在接收到ping信号时对其进行应答。但是，如果发送节点不是集群的一部分，则接收节点将丢弃所有数据包。

一个节点将仅有两种方式接受另一个节点作为集群的一部分:

*  节点发出meet假消息。meet消息非常类似ping消息，但是会强制接收者接受该节点作为集群的一部分。只有在系统管理员通过此CLUSTER MEET ip port命令请求时，节点才会向其它节点发送MEET消息。

* 如果一个节点已经被集群中的节点接受，通过gooip协议传播后也会被剩余节点接受。如A知道B，B知道C，最终B发送gossip消息到A和C,那么A将会知道C并尝试和它建立连接。

这意味着只要我们连接了集群中的任何一个节点，最终将将会自动与其它节点连接。

这种机制使集群更加健壮，可以防止不同的Redis集群在IP地址更改或其它网络相关事件发生后意外混合。


## 重定向和重新分片

### 重定向

Redis客户端可以自由地向集群中的每个节点发送命令包括从节点，节点将会分析key计算slot，查找slot所属于的节点，如果是当前节点则直接发回，否则根据内部的slot与节点的映射关系将client重定向正确的节点。返回值个事例如下:

    GET x
    -MOVED 3999 127.0.0.1:6381

返回的error包括key所属于的slot和正确节点的ip和端口。如果在重新连到新节点之前，集群的配置已经发生变换，则可能需要继续重定向。

clint必须支持-ASK类型的重定向才是一个完整的Redis集群客户端。

### 集群实时重新配置

Redis集群支持在运行中添加和移除节点。添加或删除节点被抽象到相同的操作中:移动hash slot从一个节点到另一个节点。这意味着可以使用相同的机制来重新平衡集群、添加或删除节点等等。

* 添加新节点到集群中将会把一部分slot移动到新节点
* 移除节点将会把其上的slot移动到集群中的其它节点上
* 为了重新平衡集群在节点间移动slot

实现的核心是移动hash slot。从实际角度看slot就是一些key的集合，因此移动slot到其它节点就是移动key到其它节点。

有一些CLUSTER命令用于管理集群slot的变化。如下:

* CLUSTER ADDSLOTS slot1 \[slot2]...\[slotN]
* CLUSTER DELSLOTS slot1 \[slot2]...\[slotN]
* CLUSTER SETSLOT slot NODE node
* CLUSTER SETSLOT slot MIGRATING node
* CLUSTER SETSLOT slot IMPORTING node

前两个命令的用于在节点上分配和移动slot。节点上slot发生变换后，将使用gossip协议在集群中传播。

SETSLOT字命令用于给指定节点分配一个slot。其还有2个特殊的状态MIGRATING和IMPORTING用于从一个往另一个节点迁移hash slot。

* 当slot被标记为MIGRATING，节点接收到这个slot的查询命令时，如果发现key不存在则将使用-ASK错误回复client让其重定向到迁移到的目标节点处。

* 当slot被标记为IMPORTING，只有在前一个请求是ASKING命令时才接收请求，否则会使用-MOVED将client重定向到slot的拥有者处。

让我们用一个slot迁移的例子来更清楚地说明这一点，假设我们有两个Redis主节点分别称为A和B，我们想把哈希槽8从A移到B，所以我们发出这样的命令:

* 向B发送:CLUSTER SETSLOT 8 IMPORTING A
* 向A发送:CLUSTER SETSLOT 8 MIGRATING B

查询slot 8的请求将继续发送到A节点:

* 所有A节点上存在的key都有A处理
* 所有A节点上不存在的key则将重定向到B处理

这样不会再A节点上再创建与slot 8相关的新key，将交由B处理。有一个脚本redis-trib可用于从A节点向B节点迁移slot 8，使用了如下命令实现:

    CLUSTER GETKEYSINSLOT slot count

以上命令返回指定slot中count个key。对于每个返回的key，redis-trib向节点A发送MIGRATE命令用于原子的从A节点迁移key到B节点。MIGRATE命令格式如下:

    MIGRATE target_host target_port key target_database id timeout

MIGRATE命令连接目标节点，发送序列化后的key，成功后返回OK，被迁移的key将会在被迁移的节点上将会被删除。从外部客户端的角度看key不是在A上就是在B上。

在Redis集群中，不需要指定0以外的数据库，但是MIGTATE是一个通用命令，不仅仅用于Redis集群。 MIGRAT即使在移动复杂的键时，也会尽可能快地对迁移进行优化，但是在Redis集群中，如果使用数据库的应用程序存在延迟约束，则在存在大键的地方重新配置集群并不被认为是一个明智的过程。

当迁移过程最终完成时，向迁移过程中涉及到的节点发送SETSLOT \<slot> NODE \<node-id>命令将状态修改为正常。通常将相同的命令发送给所有其他节点，以避免在集群中等待新配置的自然传播。

### ASK重定向

在前一节中，我们简要地讨论了ASK请求重定向，为什么不能直接使用MOVED重定向呢？MOVED意味着slot已经被转移到新的节点下次应该直接访问相应的节点，ASK意味着仅仅将这次的请求重定向新的节点。

下一次的请求的key可能还在A中所以需要的是临时重定向，所以我们总是先尝试A在发送到B。由于这种情况只发生在可用的16384个哈希槽中的一个哈希槽上，所以集群上的性能下降是可以接受的。

从客户端的角度来看，请求重定向的完整语义如下:

* 如果收到ASK响应，仅重定向这次的命令到新节点，之后的命令仍发送到旧的节点
* 使用ask命令启动重定向查询
* 不要更新本地客户端映射关系以将slot 8映射到B

一旦slot 8迁移完成，A将发送MOVED给客户端，客户端将永久将slot 8映射到新的节点。

### 客户端首次连接和处理重定向

为了提升性能，Redis集群客户端应尝试记录slot的配置，但是不要求最新的，在发生重定向后更新即可。

有两种场景，client需要获取slot和节点的映射关系:

* 启动时初始化slot
* 收到MOVED重定向响应 

client收到MOVED重定向命令后更新slot与节点的关系，但是这通常不是高效的，因为通常一次修改多个slot的配置(例如从节点升级为master，旧的master的上的slot都需要重映射)，这种情况下，通过重新获取slot和节点的完整映射应该是更好的方式。

Redis集群提供了CLUSTER NODES命令用于获取slot配置，返回数据格式如下:

    127.0.0.1:30006> CLUSTER Nodes
    33b6cbe0d61952405d92c6e9f8effc7c5ef4b77f 127.0.0.1:30003@40003 master - 0 1579526250085 3 connected 10923-16383
    3dae01f8f2443a18d6852abd85b252605ecee84d 127.0.0.1:30006@40006 myself,slave 33b6cbe0d61952405d92c6e9f8effc7c5ef4b77f 0 1579526250000 6 connected
    5fde9ffe782952e9d985ce4d0df71564b33ec7e5 127.0.0.1:30004@40004 slave e221f841cfad5b4bb305252ecbc784e677101d66 0 1579526250187 4 connected
    355acea8689b70f444ea3f57929b6664f6752787 127.0.0.1:30005@40005 slave 05688f432cde848749a5662004b954c939191fba 0 1579526250085 5 connected
    05688f432cde848749a5662004b954c939191fba 127.0.0.1:30002@40002 master - 0 1579526250000 2 connected 5461-10922
    e221f841cfad5b4bb305252ecbc784e677101d66 127.0.0.1:30001@40001 master - 0 1579526250085 1 connected 0-5460

新命令CLUSTER SLOTS返回slot范围数组，以及服务于指定范围的相关主节点和从节点。

下面是CLUSTER SLOTS输出的一个例子:

    127.0.0.1:7000> cluster slots
    1) 1) (integer) 5461
    2) (integer) 10922
    3) 1) "127.0.0.1"
        2) (integer) 7001
    4) 1) "127.0.0.1"
        2) (integer) 7004
    2) 1) (integer) 0
    2) (integer) 5460
    3) 1) "127.0.0.1"
        2) (integer) 7000
    4) 1) "127.0.0.1"
        2) (integer) 7003
    3) 1) (integer) 10923
    2) (integer) 16383
    3) 1) "127.0.0.1"
        2) (integer) 7002
    4) 1) "127.0.0.1"
        2) (integer) 7005

返回的数组中每项的前两个子元素是slot的起始范围，其它的是节点的地址，第一个是slot所属的master节点地址，剩余的是处于非失败状态的从节点地址。

如果集群配置错误，CLUSTER SLOTS不保证返回的slot范围覆盖所有的16384个slot。所以client应该初始化slot配置映射，用NULL对象填充目标节点，如果用户尝试执行key属于未分配slot的命令则报告错误。

在发现slot未分配时向调用方返回错误之前，客户端应再次尝试获取slot配置，以检查群集现在是否配置正确。

### 多key操作

通过使用hash tag，client可以使用多key操作，如以下的操作是有效的:

    MSET {user:1000}.name Angela {user:1000}.surname White

当多key操作涉及的slot正在进行重新分片，多key操作可能不可用。

操作不存在key或在重新分片过程中key分布到源和目标节点上，此时将会产生一个-TRYAGAIN错误。client稍后可以在尝试或者返回错误。

一旦slot的迁移完成，所有的多key操作就可以使用了。

### 使用从节点扩展读

正常情况下从节点自动重定向client请求到对应master节点，但是通过使用READONLY命令可以让client使用从节点进行扩展读操作。

READONLY告诉集群从节点，client可以接受获取过时的数据并且不用于写操作。

当连接处于只读模式时，集群只有当操作的key不属于从节点所对应的master负责的slot时，才让client进行重定向操作。可能发生的原因如下:

1. client发送的命令所属的slot不是slave对应的master所负责的
2. 集群被重新配置(例如重新分片)，slave不在负责之前对应的slot

当发生以上情况时，客户端需要更新配置。

可以使用READWRITE命令移除只读状态。

## 容错

### 心跳和gossip消息

Redis集群节点持续交换ping和pong包，这两种包具有相同的结构，并且都带有重要的配置信息，唯一实际的区别是消息类型字段，我们将把ping和pong包合称为心跳包。

通常节点发送ping包，这将触发接收者使用pong包进行应答，但也不一定完全是这样，节点也可以只发送pong包向其它节点发送关于其配置的信息。这很有用，例如，为了尽快传播一个新的配置。

通常一个节点每秒钟会ping几个随机节点，这样每个节点发送的ping包(和接收的pong包)总数是一个常量，而与集群中的节点数量无关。

然而，每个节点确保ping其它没有发送ping或接收到pong的节点时间不超过NODE_TIME的一半。在NODE_TIMEOUT超时之前，节点还尝试将TCP链接重新连接到另一个节点，以确保节点不会因为当前TCP连接存在问题而被认为是不可到达的。

如果NODE_TIMEOUT设置的比较小同时节点的数量比较多时，集群间信息的交换量可能相当大。因为为在NODE_TIMEOUT的一半时间内，每个节点都将尝试ping其它没有新信息的节点。

例如有一个100个节点的集群，NODE_TIMEOUT设置为60秒，每个节点将尝试每30秒发送99个ping。ping总数为每秒3.3次，乘以100个节点，即整个集群中每秒330个ping。

有一些方法可以减少消息的数量，但是目前还没有关于由于带宽导致的Redis集群故障，所以目前使用的是这种直接设计。即使在上面的例子中，每秒交换的330个数据包也被平均分配给100个不同的节点，因此每个节点接收到的流量是可以接受的。

### 心跳包内容

ping包和pong包含一个对所有类型的包都通用的head，还有一个ping和pong需要使用的Gossip部分。

通用head包含以下信息:

* Node ID，一个160位的伪随机字符串，在第一次创建节点时分配，并且在Redis集群节点的整个生命周期内都保持不变
* 发送节点的currentEpoch和configEpoch字段，用于Redis集群的分布式算法(在下一节有介绍)，如果节点是一个从节点，则configEpoch是已知最新的其主节点的configEpoch
* node flags，标记节点是主节点还是从节点等等
* 节点负责的slot的bitmap，如果是从节点则是其对应主节点的bitmap
* 节点端口(用于集群间交换数据的端口)
* 从发送者的角度来看集群的状态(down或ok)
* 发送节点的主节点ID(如果它是一个从节点)

ping和pong包还有gossip部分，这部分用于向接收者提供发送者角度看到的集群节点。gossip部分只包含发送者所知道的节点集中的几个随机节点的信息，gossip部分包含的节点数据与集群大小成比例。

对于gossip部分中添加的每个节点，将包含以下字段:
* Node ID
* 节点IP和端口
* Node falgs

gossip部分允许接收节点从发送方的角度获取关于其它节点状态的信息，这对于故障检测和发现集群中的其它节点都很有用。

### 故障检测

Redis集群故障检测用于识别主节点或从节点何时不能被大多数节点访问，然后通过将一个从节点提升为主节点来修复。当无法从提升时，集群将处于错误状态，停止接收来自客户端的操作。

如前面所述，每个节点都有一个与其它已知节点相关联的flag列表。这里有两个用于故障检测的标志，PFAIL和FAIL。PFAIL是指可能失败，FAIL表示一个节点发生故障，并且这个情况在一定的时间内得到了大多数master节点的确认。

PFAIL标志:

当超过NODE_TIMEOUT后还无法访问某节点，节点使用PFAIL标志标记这个无法访问的节点。主节点和从节点都可以标记为其它节点为PFAIL。

对于一个Redis集群节点来说不可达性的概念是指我们有一个活动的ping(我们发送的一个ping，我们还没有收到回复)等待的时间超过NODE_TIMEOUT还未得到回复。对于这种机制NODE_TIMEOUT必须比网络往返时间大。为了在正常操作期间增加可靠性，节点将尝试在NODE_TIMEOUT过去一半还没有得到ping的响应时将重新连接集群中的其它节点。该机制确保连接处于活动状态，因此断开的连接通常不会导致节点之间出现错误的故障报告。

FAIL标志:

PFAIL标志仅仅是每个节点对于其它节点的状态的标志，不足以触发一个从节点升级。

如本文节点心跳部分所述，每个节点随机的向其已知的其它节点发送gossip信息时会包含节点的状态，每个节点最终都会收到一组其它节点的节点标志。这样每个节点都有一种机制，向其它节点发送它们检测到的故障情况的信息。

当满足下列条件时，PFAIL将升级为FAIL:

* 我们称某个节点为A，另一个节点B标记为了PFAIL
* 节点A通过gossip收集来来自于集群中其它大多数master节点关于B的信息
* 在NODE_TIMEOUT * FAIL_REPORT_VALIDITY_MULT(在当前实现中，有效性因子被设置为2，因此这是NODE_TIMEOUT时间的两倍)时间范围内，大多数节点将其标记为了PFAIL或FAIL
* 将节点标记为FAIL
* 向所有可到达的节点发送FAIL消息

FAIL消息将强制每个接收节点将节点标记为FAIL状态，不论是否节点被标记为了PFAIL状态。

FAIL标记的移除需要满足以下条件之一:

* 节点是可访问的并且是一个从节点。在这种情况下，失败标志可以被清除，因为从服务器没有失败转移
* 节点已经可访问并且是不负责任何slot的master节点。在这种情况下，可以清除FAIL标志，因为没有slot的master并不真正参与集群，而是等待被配置以加入集群
* 节点已经可访问并且是一个主节点，但是很长一段时间(NODE_TIMEOUT的N倍)过去了，没有任何可检测到的从节点提升。在这中情况下该节点需要重新加入集群

需要注意的是，虽然PFAIL -> FAIL转换使用一种协议形式，但是使用的协议是弱协议:

1. 节点收集某个时间段内其它节点的视图，因此即使需要大多数master节点“同意”，实际上这只是我们在不同的时间从不同的节点收集的节点状态信息，我们不确定也不需要在给定的时刻大多数master节点同意。

2.每个被标记为fail的节点将使用fail消息强制集群中的其它节点接受，无法确保消息到达所有节点。例如，一个节点可能处于fail状态，但是由于分区的原因无法到达任何其它节点

Redis集群故障检测有一个存活要求:最终所有节点都应该同意给定节点的状态。有两种情况可能源于脑裂，要么少数节点认为该节点处于失败状态，要么少数节点认为该节点不处于失败状态，在这两种情况下，集群最终都将拥有一个给定节点状态的单一视图:

情况1:如果由于故障检测和它所产生的连锁效应，如果大多数master节点已经将某个节点标记为fail，那么其它节点最终都会将该master节点标记为fail，因为在指定的时间窗口内将报告足够的失败。
情况2:当只有少数master节点将某个节点标记为fail时，将不会发生从节点升级(因为它使用了一种更正式的算法，以确保每个节点最终都知道升级了)。每个节点都将按照上面的fail状态清除规则来清除fail标记(即在经过N次NODE_TIMEOUT之后没有提升)

FAIL标志仅用作用于安全运行从节点提升算法的触发器。从理论上讲，一个从节点独立行动并在它的master节点不能到达的时候开始一个从节点提升，如果master实际上是大多数节点都能到达的将返回拒绝。虽然增加了PFAIL ->FAIL状态的复杂性，协议的一致性较弱，在短时间内fail消息要传播到可达的节点。由于这些机制，如果集群处于错误状态，通常所有节点将在同一时间停止接受写操作。从使用Redis集群的应用程序的角度来看，这是一个理想的特性。此外，还避免了从节点发起的错误的选举尝试，这些从节点由于本地问题而无法访问其主节点(其它大多数主节点都可以访问master节点)。

## 配置处理、传播和故障转移

### 集群当前的epoch

Redis集群使用了一个类似Raft算法中的"任期"概念。在Redis集群中，这个术语被称为纪元epoch，它用于对事件进行增量版本控制。当多个节点提供相互冲突的信息时，另一个节点就有可能知道哪个状态是最新的。

currentEpoch是一个64位无符号数字。

在节点创建时，每个Redis集群节点(从节点和主节点)都将currentEpoch设置为0。

每次从另一个节点接收包时，如果发送方的epoch(集群总线消息头的一部分)大于本地节点的epoch，则currentEpoch将更新为发送方的epoch。

由于这些语义，最终所有节点都将同意集群中最大的configEpoch。

当集群的状态发生变化，节点为了执行某些操作而寻求协议时，将使用此信息。

目前只在从节点提升期间使用，如下一节所述。基本来说，epoch是集群的逻辑时钟，将采用epoch更大的信息。

### 配置的epoch

每一个master节点总是通过ping包和pong包告诉其它节点自己的configEpoch和负责的slot。

创建新节点时，master中的configEpoch设置为0。

在从节点选举期间创建一个新的configEpoch。从节点尝试取代失败的主节点增加epoch并尽量获得大多数master节点的授权。当从节点被授权，创建一个新的唯一的configEpoch，使用这个新的configEpoch，slave升级为了master节点。

正如在下一节中所解释的，configEpoch有助于解决不同节点声明不同配置时的冲突(由于网络分区和节点故障而可能发生的一种情况)。

从节点也在ping和pong包中声明configEpoch字段，但在从节点的情况下，该字段表示其主节点在最后一次交换包时的configEpoch。这允许其它实例检测从节点何时拥有需要更新的旧配置(主节点不会将选票授予拥有旧配置的从节点)。

每次某个已知节点的configEpoch更改时，它都会被接收此信息的所有节点永久地存储在nodes.conf文件中。对于currentEpoch也是如此。这两个变量被保证在节点继续操作之前先保存并同步到磁盘。

configEpoch的值是使用简单算法在故障转移期间生成的，保证是新的、增量的和惟一的。

### 从节点选举和提升

从节点处理从节点的选举和提升，主节点投票给从节点进行提升。从节点的角度看到它的master节点处于FAIL状态就开始进行选举。

从节点为了提升为主节点，它需要发起选举并赢得选举。当主节点处于FAIL状态时，它所有的从节点都可以发起选举，然而只有一个从节点会赢得选举并提升为主节点。

当满足下列条件时，从节点开始选举:

* 从节点的主节点处于FAIL状态
* 主节点拥有负责的slot
* 主从复制断开的时间不超过给定值，为了确保提升的从节点的数据足够新，这个阈值是用户可配置的

为了被选中，从节点的第一步是增加它的currentEpoch计数器，并请求主节点投票。

从节点请求投票时，通过向集群的每个主节点广播一个FAILOVER_AUTH_REQUEST消息。然后从节点将等待主节点的回复，最大等待时间为NODE_TIMEOUT的2倍(但总是至少2秒)。

一旦主节点为投票给了某个从节点，并使用FAILOVER_AUTH_ACK进行积极响应，那么在NODE_TIMEOUT * 2的时间段内，它就不能再为同一个主节点的另一个从节点进行投票了。在此期间它将无法回复同一主节点的其它授权请求。这不是为了保证安全，而是为了防止多个从节点同时当选(即使是不同的选举)，这通常是不需要的。

从节点丢弃任何AUTH_ACK应答，如果应答的epoch小于发送投票请求时的currentEpoch。这就保证了它不会计算前一次选举的选票。

一旦从节点从多数master节点那里得到选票，它就赢得了选举。否则，如果在2*NODE_TIMEOUT(至少2秒)期间内没有达到多数，则选举将被中止，并且在NODE_TIMEOUT * 4之后将再次尝试一个新的选举(并且总是至少4秒)。

### 从节点等级

一旦master处于FAIL状态，从节点在试图发起选举之前会等待一段时间。延迟的计算如下:

    DELAY = 500 milliseconds + random delay between 0 and 500 milliseconds +
            SLAVE_RANK * 1000 milliseconds.

固定的延迟确保我们等待FAIL状态在集群中传播，否则，如果从节点发起选举时，其它主节点还未知道这个主节点失败则将拒绝投票。

随机延迟被用来取消从节点的同步，所以他们不太可能同时开始选举。

SLAVE_RANK是从节点的等级与它从主节点处理的复制数据的数量有关。

当主节点失败时，从节点交换信息以建立顺序:拥有最大复制偏移的排在最前面，以此类推。通过这种方式，拥有最新数据的从节点在其它节点之前先发起选举。

等级次序不严格，如果一个等级高的从节点没有被选上，其它的从节点很快就会尝试。

一旦一个从节点赢得了选举，它就会获得一个新的唯一的和递增的比任何其它现有的主节点更高的configEpoch。它将会通过ping和pong信息将它的configEpoch和其所负责的slot广播给其它节点。

为了加速其它节点的重新配置，一个pong包被广播到集群的所有节点。

当前不可到达的节点最终将在从另一个节点接收到ping或pong包时重新配置，或者在检测到节点通过心跳包发布的信息过期时，将收到从另一个节点发的更新包。

其它节点将检测到有一个拥有更大configEpoch新主节点与旧主节点负责相同的slot，将会更新配置。旧主节点的从节点(或重新加入集群的被失败转移的主节点)不仅会升级配置，还会重新配置以从新的主节点进行复制

### 主节点响应从节点的投票请求

在前面的章节中，我们讨论了从节点是如何被选举的。本节将从请求为给定的从节点进行投票的主节点的角度解释所发生的事情。

主节点收到来自从节点的FAILOVER_AUTH_REQUEST授权请求。

要使投票获得通过，必须满足下列条件:

1. master节点只对一个特定的epoch投一次票，而拒绝对更早的epoch进行票:每个master节点都有一个lastVoteEpoch字段，只要授权请求包中的currentEpoch不大于lastVoteEpoch，主节点就会拒绝投票。当主节点对投票请求做出响应时，lastVoteEpoch将相应地更新，并安全地保存到磁盘上。

2. 只有在从节点的master被标记为FAIL时，master才会投票给从节点

3. 授权请求中的currentEpoch如果小于主节点的currentEpoch的则会被忽略。因为主节点的回复将具有与授权请求相同的currentEpoch(最新的epoch)，如果同一个从节点再次请求投票，从节点的currentEpoch会增加，这就保证了主节点的延迟回复不会被从节点新发起的投票请求接受。

不使用规则3导致的问题示例如下:

主节点currentEpoch是5，lastVoteEpoch是1(这可能发生在几次失败的选举之后)

* 从节点currentEpoch是3
* 从节点发起授权请求epcoh是4(3+1),主节点用currentEpoch 5回复ok，此时响应延迟
* 从节点再次尝试选举，在一段时间后，用epoch 5(4+1)发起请求，此时延迟的响应到达从节点currentEpoch 5并且能够被接受。加上新发起的投票请求的响应从节点重复获了投票。

主机点的投票规则:

1. 在NODE_TIMEOUT * 2结束之前，如果已经投票给主节点的从节点，那么该主节点将不会在投票给其它从节点。这并不是严格要求的，因为两个从节点不可能在同一epoch赢得选举。然而，在实践中，它可以确保当一个从节点被选上时，它有足够的时间通知其它从节点，并避免另一个从节点赢得新选举的可能性从而执行不必要的第二次故障转移。

2. 主节点不会去选择最合适的的从节点。如果从节点的主节点处于失败状态，并且主节点在当前的epoch内没有投票，则可以投票。最好的从节点是最有可能开始选举并在其它从节点之前赢得选举的，因为它的等级更高，通常能够更早地开始投票过程，如前一节所述。

3. 从节点负责的slot所处的configEpoch比主节点已知的负责这些slot的节点所处的configEpoch小时，主节点不会进行投票。需要注意的是，从节点发送的是它的主节点的configEpoch和slot。


### 在分区期间配置epoch的实际示例











       














